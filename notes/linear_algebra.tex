\documentclass[a4paper,10pt]{article}
\usepackage{mystyle}

\begin{document}

\begin{defn}[Echelon Form]
	A matrix is in Echelon form if:
	\begin{enumerate}
		\item all non-zero rows come before zero rows
		\item below each pivot is a column of zeros
		\item each pivot is to the right of the previous pivot
	\end{enumerate}

	If each pivot is $1$ then the matrix is in row echelon form and if
	there is a column of zeros above each pivot, the matrix is in reduced
	row echelon form (RRE).
\end{defn}

\begin{defn}[Rank]
	Let $A$ be an $n \times m$ matrix. Its rank is equal to the number of
	non-zero rows in its echelon form. This is denoted by $\rho(A)$.
\end{defn}

\begin{defn}[Subspace]
	Let $U \subset \RR^n$. $U$ is a subspace if:
	\begin{enumerate}
		\item $\bm{0} \in U$
		\item $U$ is closed under addition and scalar multiplication
	\end{enumerate}
\end{defn}

\begin{defn}[Vector Space]
	$V$ is a vector space of a field $\FF$ if:
	\begin{enumerate}
		\item addition is commutative and associative
		\item neutral and inverse additive elements exist
		\item scalar multiplication is commutative and associative
		\item $1$ is the neutral scalar
		\item $(a+b)\bm{d} = a\bm{d} + b\bm{d} \quad \forall a,b \in \FF, \bm{d} \in V$
		\item $(\bm{a} + \bm{b})d = \bm{a}d + \bm{b}d \quad \forall \bm{a},\bm{b} \in V, d \in \FF$
	\end{enumerate}
\end{defn}

\begin{defn}[Null Space]
	If $A \in M_{m \times n}(\RR)$, the null space is defined as:
	\[
		N(A) = \{ \bm{x} \in \RR^n \mid A\bm{x} = \bm{0} \}
	\]
\end{defn}

\begin{prop}
	If $A \in M_{m \times n}(\RR)$, the null space $N(A)$ is a
	subspace of $\RR^n$.
\end{prop}

\begin{proof}
	It is clear that $\bm{0} \in N(A)$. It remains to be shown
	that addition and scalar multiplication are closed.

	Let $x,y \in \RR^n$. We know that $\RR^n$ is a vector space,
	hence $A(x+y) = Ax + Ay = \bm{0}$.

	Let $x \in \RR^n, \, a \in \RR$. $A(ax) = a(Ax) = a\bm{0} = \bm{0}$.
\end{proof}

\begin{defn}[Image Space]
	$Im A = \{ \bm{y} \in \RR^m \mid \bm{y} = A\bm{x}, \bm{x} \in \RR^n \}$
\end{defn}

\begin{prop}
	$Im A$ is a subspace of $\RR^m$.
\end{prop}

\begin{proof}
	We know that $\bm{0} \in \RR^n$ hence $A\bm{0} = 0 \in Im A$.

	Let $y_1 = Ax_1, y_2 = Ax_2 \in \RR^m$.
	\[
		y_1 + y_2 = Ax_1 + Ax_2 = A(x_1 + x_2)
	\]
	\[
		x_1 + x_2 \in \RR^n \Rightarrow y_1 + y_2 \in Im A
	\]

	Let $k \in \RR, y \in Im A$.
	\[
		ky = kAx = Akx
	\]
	\[
		kx \in \RR^n \Rightarrow ky \in Im A
	\]
\end{proof}

\begin{ex}
	Find $N(A)$ and $Im(A)$ where
	\[
		A =
		\begin{pmatrix}
			1  & 3 & -2 \\
			-2 & 4 & 4
		\end{pmatrix}
	\]

	First we find the RRE of $A$:
	\[
		\begin{pmatrix}
			1  & 3 & -2 \\
			-2 & 4 & 4
		\end{pmatrix}
		=
		\begin{pmatrix}
			1 & 3  & -2 \\
			0 & 10 & 0
		\end{pmatrix}
		=
		\begin{pmatrix}
			1 & 3 & -2 \\
			0 & 1 & 0
		\end{pmatrix}
		=
		\begin{pmatrix}
			1 & 0 & -2 \\
			0 & 1 & 0
		\end{pmatrix}
	\]

	$N(A)$:
	\[
		\begin{pmatrix}
			1 & 0 & -2 \\
			0 & 1 & 0
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
			x_3
		\end{pmatrix}
		=
		\begin{pmatrix}
			0 \\
			0 \\
			0
		\end{pmatrix}
	\]
	\[
		\Rightarrow x_2 = 0, \quad x_1 = 2x_3
	\]
	\[
		\Rightarrow N(A) = k
		\begin{pmatrix}
			2 \\
			0 \\
			1
		\end{pmatrix}
		, k \in \RR
	\]

	$Im(A)$:
	\[
		\begin{pmatrix}
			y_1 \\
			y_2 \\
			y_3
		\end{pmatrix}
		=
		\begin{pmatrix}
			1  & 3 & -2 \\
			-2 & 4 & 4
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
			x_3
		\end{pmatrix}
		=
		\begin{pmatrix}
			x_1   + 3x_2 - 2x_3  \\
			-2x_1 + 4x_2 + 4x_3
		\end{pmatrix}
	\]
	\[
		= x_1
		\begin{pmatrix}
			1 \\
			-2
		\end{pmatrix}
		+ x_2
		\begin{pmatrix}
			3 \\
			4
		\end{pmatrix}
		+ x_3
		\begin{pmatrix}
			-2 \\
			4
		\end{pmatrix}
	\]

	Note that the vectors in this expression are simply the columns of $A$, in fact in general:
	\[ Im(A) = \{x_1 a_{i1} + \cdots + x_1 a_{in}\} \]
	Hence the image space is sometimes referred to as the column space.
\end{ex}

Another example of a vector space is the space of polynomials over a field $\FF$:
\[
	\FF[x] =
	\{a_0 + a_1 x + \cdots + a_n x^2 \mid a_i \in \FF, n \in \NN_0\}
\]
e.g. $\RR[x]$ is a vs over $\RR$.
Addition and scalar multiplication work as expected.

We can also define a vs on a set of funtions:
\[
	V = \{f:[0,1] \to \RR\}
\]
Addition:
\[
	(g+f)(x) = g(x) + f(x)
\]
Scalar multiplication:
\[
	(\alpha f)(x) = \alpha (f(x)), \alpha \in \RR
\]

% TODO: prove these examples ^^.

\begin{defn}[Transpose]
	If $A = a(i,j) \in M_{m \times n}(\RR)$, then its transpose is
	\[ A^T = a(j,i) \]
	i.e its columns are swapped with its rows.
\end{defn}
\begin{defn}[Symmetric]
	A matrix $A \in M_{n}(\RR)$ is called symmetric iff
	\[ A^T = A. \]
\end{defn}

Properties of $A^T$:
\[
	{(A^T)}^T = A
\]
\[
	{(A+B)}^T = A^T + B^T
\]
\[
	{(AC)}^T = C^T A^T
\]

Spanning sets, linear independence, bases:

\begin{defn}[Linear Combination]
	Let $V$ be a vector space over $\RR$. A linear combination of vectors in $V$ is given as:
	\[
		\alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_r v_r, \quad
		\alpha_i \in \RR, \, v_i \in V
	\]
\end{defn}

\begin{prop}
	$U = \{
		\alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_r v_r \mid
		\alpha_i \in \RR, \, v_i \in V
	\}$ is a subspace.
\end{prop}

\begin{proof}
	\[
		\alpha_i = 0 \, \forall i \Rightarrow 0 + \cdots + 0 = 0
	\]
	\[
		\Rightarrow \bm{0} \in U
	\]
	\[
		a = \sum_{i=0}^r \alpha_i v_i \quad
		b = \sum_{i=0}^r \beta_i v_i \\
	\]
	\begin{align*}
		\Rightarrow a + b &= \sum (\alpha_i v_i + \beta_i v_i) \\
		&= \sum (\alpha_i + \beta_i) v_i \in U
	\end{align*}

	\[
		a = \sum \alpha_i v_i \quad \lambda \in \RR
	\]
	\[
		\lambda a = \sum \lambda \alpha_i v_i \quad
		\lambda \alpha_i \in \RR \, \forall i
		\Rightarrow \lambda a \in U
	\]
\end{proof}

\begin{defn}[Spanning Set]
	Let $V$ be a vector space and $W$ be a subspace. A set of vectors
	$\{v_1, \ldots, v_r\} \subset V$ is a spanning set for $W$ if
	\[
		W =
		\{\alpha_1 v_1 + \cdots + \alpha_r v_r \mid \alpha_i \in \RR\}
		= sp\{v_1, \ldots, v_r\}
	\]
	We say that $\{v_1, \ldots, v_r\}$ spans $W$.
\end{defn}

\begin{ex}
	If $A = \begin{pmatrix}
		1  & 3 & -2 \\
		-2 & 4 & 4
	\end{pmatrix}$ we know that
	\[
		Im(A) = x_1
		\begin{pmatrix}
			1 \\
			-2
		\end{pmatrix}
		+ x_2
		\begin{pmatrix}
			3 \\
			4
		\end{pmatrix}
		+ x_3
		\begin{pmatrix}
			-2 \\
			4
		\end{pmatrix}
	\]

	We can also say that
	\[
		Im(A) = sp\left\{
			\begin{pmatrix}
				1 \\
				-2
			\end{pmatrix}
			,
			\begin{pmatrix}
				3 \\
				4
			\end{pmatrix}
			,
			\begin{pmatrix}
				-2 \\
				4
			\end{pmatrix}
		\right\}
	\]
	since every element in $Im(A)$ is a linear combination of these vectors.
\end{ex}

\begin{ex}
	\[
		\left\{
			\begin{pmatrix}
				1 \\
				0 \\
				0
			\end{pmatrix}
			,
			\begin{pmatrix}
				0 \\
				1 \\
				0
			\end{pmatrix}
			,
			\begin{pmatrix}
				0 \\
				0 \\
				1
			\end{pmatrix}
		\right\}
	\]
	spans $\RR^3$.
\end{ex}

\begin{ex}
	Let
	\[
		W = \left\{
			\begin{pmatrix}
				a + b + 2c \\
				b + c \\
				a + c
			\end{pmatrix}
			\mid a,b,c \in \RR
		\right\}
		\subset \RR^3
	\]
	then
	\[
		w = a
		\begin{pmatrix}
			1 \\
			0 \\
			1
		\end{pmatrix}
		+ b
		\begin{pmatrix}
			1 \\
			1 \\
			0
		\end{pmatrix}
		+ c
		\begin{pmatrix}
			2 \\
			1 \\
			1
		\end{pmatrix}
		\forall w \in W
	\]
	\[
		\Rightarrow W = sp\left\{
			\begin{pmatrix}
				1 \\
				0 \\
				1
			\end{pmatrix}
			,
			\begin{pmatrix}
				1 \\
				1 \\
				0
			\end{pmatrix}
			,
			\begin{pmatrix}
				2 \\
				1 \\
				1
			\end{pmatrix}
		\right\}
	\]

	Also, since
	\[
		\begin{pmatrix}
			2 \\
			1 \\
			1
		\end{pmatrix}
		=
		\begin{pmatrix}
			1 \\
			0 \\
			1
		\end{pmatrix}
		+
		\begin{pmatrix}
			1 \\
			1 \\
			0
		\end{pmatrix},
	\]
	\[
		W = sp\left\{
			\begin{pmatrix}
				1 \\
				0 \\
				1
			\end{pmatrix}
			,
			\begin{pmatrix}
				1 \\
				1 \\
				0
			\end{pmatrix}
		\right\}
	\]
\end{ex}

\begin{defn}[Linear Independence]
	Let $V$ be a vector space. A set of vectors
	$\{v_1, \ldots, v_r \mid v_i \in V \}$
	is linearly independent if
	\[
		\alpha_1 v_1 + \cdots + \alpha_r v_r = 0
		\implies \alpha_1 = \cdots = \alpha_r = 0.
	\]
\end{defn}

\begin{thm}
	Let $v_1, \ldots, v_r \in \RR^n$ and $A$ be the matrix whose
	columns correspond to the vectors $v_i$, then the set
	$\{v_1, \ldots, v_r\}$ is linearly independent iff
	$A\bm{x} = 0$ has the unique solution $x = 0$ iff $\rho(A) = r$.
\end{thm}

\begin{proof}
	By definition of linear independence,
	\[
		x_1 v_1 + \cdots + x_r v_r = 0
		\iff
		x_i = 0 \, \forall i
	\]
	This is clearly equivalent to saying that $\bm x$ is the
	only solution to $A\bm x = 0$.

	A matrix equation like this can have a unique solution iff
	$\rho(A) = r$, otherwise there would be free variables resulting
	in a range of solutions.
\end{proof}

We now know that any matrix whose rank is equal to the number of its
rows forms a linearly independent spanning set for a subspace of
$\RR^n$.

\begin{cor}
	Any set of $m > n$ vectors in $\RR^n$ is linearly dependent.
\end{cor}

\begin{cor}
	The columns of the indentity matrix $I_n$ form a linearly
	independent spanning set of $\RR^n$.
\end{cor}

\begin{defn}[Basis]
	A subset $B$ of a vector space $V$ is called a basis of $V$ if $B$ is linearly independent and $B$ spans $V$.
\end{defn}

\begin{rem}
	When considering $\RR^n$ or its subsets, a basis will always be
	finite; however a basis can be infinite, for example
	$\{1, x, x^2, \ldots\}$ is a basis of $\RR[x]$.
\end{rem}

\begin{ex}
	\[
		\left\{
			\begin{pmatrix}
				1 \\
				2 \\
				3
			\end{pmatrix}
			,
			\begin{pmatrix}
				1 \\
				-1 \\
				0
			\end{pmatrix}
			,
			\begin{pmatrix}
				5 \\
				4 \\
				4
			\end{pmatrix}
		\right\}
	\]
	is a basis of $\RR^3$.

	\[
		\begin{pmatrix}
			1 & 1  & 5 \\
			2 & -1 & 4 \\
			3 & 0  & 4
		\end{pmatrix}
		=
		\begin{pmatrix}
			1 & 1  & 5  \\
			0 & -3 & -6 \\
			0 & -3 & -11
		\end{pmatrix}
		=
		\begin{pmatrix}
			1 & 1  & 5  \\
			0 & -3 & -6 \\
			0 & 0  & -5
		\end{pmatrix}
	\]

	We can see that the rank is $3$ hence this set is linearly independent.
	\[
		\begin{pmatrix}
			1 & 1  & 5  \\
			0 & -3 & -6 \\
			0 & 0  & -5
		\end{pmatrix}
		=
		\begin{pmatrix}
			1 & 1 & 5  \\
			0 & 1 & -2 \\
			0 & 0 & 1
		\end{pmatrix}
		=
		\begin{pmatrix}
			1 & 0 & 0 \\
			0 & 0 & 0 \\
			0 & 0 & 1
		\end{pmatrix}
	\]

	This shows that this set spans $\RR^3$ and is therefore a basis.
\end{ex}

\begin{thm}
	Let $V$ be a vector space over $\RR$. If $U = \{v_1, \ldots,
	v_r\}$ spans $V$ and no proper subset of $U$ spans $V$, then $U$
	is a basis for $V$.
\end{thm}

\begin{proof}
	We know that $U$ spans $V$ so we need only prove that $U$ is linearly independent. We will do so using the contrapositive method.

	If $U$ is not linearly independent then $\exists v_i \in U$ such that
	\[
		v_i = \alpha_1 v_1 + \cdots + \alpha_{i-1} v_{i-1} +
		\alpha_{i+1} v_{i+1} + \cdots + \alpha_r v_r
	\]
	\[
		\implies
		W = {v_1, \ldots, v_{i-1}, v_{i+1}, \ldots, v_r}
		\subset U
	\]
	spans $V$.

	Therefore, if there is no such $W$, $U$ must be linearly
	independent and hence a basis.
\end{proof}

\begin{thm}
	Let $V$ be a vector space over $\RR$. If $U = \{v_1, \ldots, v_r\}$
	is a linearly independent subset of $V$ and no proper, linearly
	independent  subset $W$ of $V$, where $W$ contains $U$, then $U$
	is a basis of $V$.
\end{thm}

\begin{proof}
	We know $U$ is linearly independent so we need only show that it
	spans $V$.

	Let $v \in V \setminus U$. According to our assumption that no
	linearly independent subset exists between $U$ and $V$, this
	means that $\{v_1, \ldots, v_r, v\}$ is  not linearly
	independent; hence any element in this set can be expressed as a
	linear combination of the rest.

	\[
		\implies
		v = \sum_{i=1}^r \alpha_i v_i \quad \forall v \in V
	\]

	Therefore, $U$ spans $V$ and $U$ is a basis for $V$.
\end{proof}

\end{document}
