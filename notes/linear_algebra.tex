\documentclass[a4paper,10pt]{article}
\usepackage{mystyle}

\begin{document}

\begin{defn}[Echelon Form]
	A matrix is in Echelon form if:
	\begin{enumerate}
		\item all non-zero rows come before zero rows
		\item below each pivot is a column of zeros
		\item each pivot is to the right of the previous pivot
	\end{enumerate}

	If each pivot is $1$ then the matrix is in row echelon form and if
	there is a column of zeros above each pivot, the matrix is in reduced
	row echelon form (RRE).
\end{defn}

\begin{defn}[Rank]
	Let $A$ be an $n \times m$ matrix. Its rank is equal to the number of
	non-zero rows in its echelon form. This is denoted by $\rho(A)$.
\end{defn}

\begin{defn}[Subspace]
	Let $U \subset \RR^n$. $U$ is a subspace if:
	\begin{enumerate}
		\item $\bm{0} \in U$
		\item $U$ is closed under addition and scalar multiplication
	\end{enumerate}
\end{defn}

\begin{defn}[Vector Space]
	$V$ is a vector space of a field $\FF$ if:
	\begin{enumerate}
		\item addition is commutative and associative
		\item neutral and inverse additive elements exist
		\item scalar multiplication is commutative and associative
		\item $1$ is the neutral scalar
		\item $(a+b)\bm{d} = a\bm{d} + b\bm{d} \quad \forall a,b \in \FF, \bm{d} \in V$
		\item $(\bm{a} + \bm{b})d = \bm{a}d + \bm{b}d \quad \forall \bm{a},\bm{b} \in V, d \in \FF$
	\end{enumerate}
\end{defn}

\begin{defn}[Null Space]
	If $A \in M_{m \times n}(\RR)$, the null space is defined as:
	\[
		N(A) = \{ \bm{x} \in \RR^n \mid A\bm{x} = \bm{0} \}
	\]
\end{defn}

\begin{prop}
	If $A \in M_{m \times n}(\RR)$, the null space $N(A)$ is a
	subspace of $\RR^n$.
\end{prop}

\begin{proof}
	It is clear that $\bm{0} \in N(A)$. It remains to be shown
	that addition and scalar multiplication are closed.

	Let $x,y \in \RR^n$. We know that $\RR^n$ is a vector space,
	hence $A(x+y) = Ax + Ay = \bm{0}$.

	Let $x \in \RR^n, \, a \in \RR$. $A(ax) = a(Ax) = a\bm{0} = \bm{0}$.
\end{proof}

\begin{defn}[Image Space]
	$Im A = \{ \bm{y} \in \RR^m \mid \bm{y} = A\bm{x}, \bm{x} \in \RR^n \}$
\end{defn}

\begin{prop}
	$Im A$ is a subspace of $\RR^m$.
\end{prop}

\begin{proof}
	We know that $\bm{0} \in \RR^n$ hence $A\bm{0} = 0 \in Im A$.

	Let $y_1 = Ax_1, y_2 = Ax_2 \in \RR^m$.
	\[
		y_1 + y_2 = Ax_1 + Ax_2 = A(x_1 + x_2)
	\]
	\[
		x_1 + x_2 \in \RR^n \Rightarrow y_1 + y_2 \in Im A
	\]

	Let $k \in \RR, y \in Im A$.
	\[
		ky = kAx = Akx
	\]
	\[
		kx \in \RR^n \Rightarrow ky \in Im A
	\]
\end{proof}

\begin{ex}
	Find $N(A)$ and $Im(A)$ where
	\[
		A =
		\begin{pmatrix}
			1  & 3 & -2 \\
			-2 & 4 & 4
		\end{pmatrix}
	\]

	First we find the RRE of $A$:
	\[
		\begin{pmatrix}
			1  & 3 & -2 \\
			-2 & 4 & 4
		\end{pmatrix}
		=
		\begin{pmatrix}
			1 & 3  & -2 \\
			0 & 10 & 0
		\end{pmatrix}
		=
		\begin{pmatrix}
			1 & 3 & -2 \\
			0 & 1 & 0
		\end{pmatrix}
		=
		\begin{pmatrix}
			1 & 0 & -2 \\
			0 & 1 & 0
		\end{pmatrix}
	\]

	$N(A)$:
	\[
		\begin{pmatrix}
			1 & 0 & -2 \\
			0 & 1 & 0
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
			x_3
		\end{pmatrix}
		=
		\begin{pmatrix}
			0 \\
			0 \\
			0
		\end{pmatrix}
	\]
	\[
		\Rightarrow x_2 = 0, \quad x_1 = 2x_3
	\]
	\[
		\Rightarrow N(A) = k
		\begin{pmatrix}
			2 \\
			0 \\
			1
		\end{pmatrix}
		, k \in \RR
	\]

	$Im(A)$:
	\[
		\begin{pmatrix}
			y_1 \\
			y_2 \\
			y_3
		\end{pmatrix}
		=
		\begin{pmatrix}
			1  & 3 & -2 \\
			-2 & 4 & 4
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
			x_3
		\end{pmatrix}
		=
		\begin{pmatrix}
			x_1   + 3x_2 - 2x_3  \\
			-2x_1 + 4x_2 + 4x_3
		\end{pmatrix}
	\]
	\[
		= x_1
		\begin{pmatrix}
			1 \\
			-2
		\end{pmatrix}
		+ x_2
		\begin{pmatrix}
			3 \\
			4
		\end{pmatrix}
		+ x_3
		\begin{pmatrix}
			-2 \\
			4
		\end{pmatrix}
	\]

	Note that the vectors in this expression are simply the columns of $A$, in fact in general:
	\[ Im(A) = \{x_1 a_{i1} + \cdots + x_1 a_{in}\} \]
	Hence the image space is sometimes referred to as the column space.
\end{ex}

Another example of a vector space is the space of polynomials over a field $\FF$:
\[
	\FF[x] =
	\{a_0 + a_1 x + \cdots + a_n x^2 \mid a_i \in \FF, n \in \NN_0\}
\]
e.g. $\RR[x]$ is a vs over $\RR$.
Addition and scalar multiplication work as expected.

We can also define a vs on a set of funtions:
\[
	V = \{f:[0,1] \to \RR\}
\]
Addition:
\[
	(g+f)(x) = g(x) + f(x)
\]
Scalar multiplication:
\[
	(\alpha f)(x) = \alpha (f(x)), \alpha \in \RR
\]

% TODO: prove these examples ^^.

\begin{defn}[Transpose]
	If $A = a(i,j) \in M_{m \times n}(\RR)$, then its transpose is
	\[ A^T = a(j,i) \]
	i.e its columns are swapped with its rows.
\end{defn}
\begin{defn}[Symmetric]
	A matrix $A \in M_{n}(\RR)$ is called symmetric iff
	\[ A^T = A. \]
\end{defn}

Properties of $A^T$:
\[
	{(A^T)}^T = A
\]
\[
	{(A+B)}^T = A^T + B^T
\]
\[
	{(AC)}^T = C^T A^T
\]

Spanning sets, linear independence, bases:

\begin{defn}[Linear Combination]
	Let $V$ be a vector space over $\RR$. A linear combination of vectors in $V$ is given as:
	\[
		\alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_r v_r, \quad
		\alpha_i \in \RR, \, v_i \in V
	\]
\end{defn}

\begin{prop}
	$U = \{
		\alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_r v_r \mid
		\alpha_i \in \RR, \, v_i \in V
	\}$ is a subspace.
\end{prop}

\begin{proof}
	\[
		\alpha_i = 0 \, \forall i \Rightarrow 0 + \cdots + 0 = 0
	\]
	\[
		\Rightarrow \bm{0} \in U
	\]
	\[
		a = \sum_{i=0}^r \alpha_i v_i \quad
		b = \sum_{i=0}^r \beta_i v_i \\
	\]
	\begin{align*}
		\Rightarrow a + b &= \sum (\alpha_i v_i + \beta_i v_i) \\
		&= \sum (\alpha_i + \beta_i) v_i \in U
	\end{align*}

	\[
		a = \sum \alpha_i v_i \quad \lambda \in \RR
	\]
	\[
		\lambda a = \sum \lambda \alpha_i v_i \quad
		\lambda \alpha_i \in \RR \, \forall i
		\Rightarrow \lambda a \in U
	\]
\end{proof}

\begin{defn}[Spanning Set]
	Let $V$ be a vector space and $W$ be a subspace. A set of vectors
	$\{v_1, \ldots, v_r\} \subset V$ is a spanning set for $W$ if
	\[
		W =
		\{\alpha_1 v_1 + \cdots + \alpha_r v_r \mid \alpha_i \in \RR\}
		= sp\{v_1, \ldots, v_r\}
	\]
	We say that $\{v_1, \ldots, v_r\}$ spans $W$.
\end{defn}

\begin{ex}
	If $A = \begin{pmatrix}
		1  & 3 & -2 \\
		-2 & 4 & 4
	\end{pmatrix}$ we know that
	\[
		Im(A) = x_1
		\begin{pmatrix}
			1 \\
			-2
		\end{pmatrix}
		+ x_2
		\begin{pmatrix}
			3 \\
			4
		\end{pmatrix}
		+ x_3
		\begin{pmatrix}
			-2 \\
			4
		\end{pmatrix}
	\]

	We can also say that
	\[
		Im(A) = sp\left\{
			\begin{pmatrix}
				1 \\
				-2
			\end{pmatrix}
			,
			\begin{pmatrix}
				3 \\
				4
			\end{pmatrix}
			,
			\begin{pmatrix}
				-2 \\
				4
			\end{pmatrix}
		\right\}
	\]
	since every element in $Im(A)$ is a linear combination of these vectors.
\end{ex}

\begin{ex}
	\[
		\left\{
			\begin{pmatrix}
				1 \\
				0 \\
				0
			\end{pmatrix}
			,
			\begin{pmatrix}
				0 \\
				1 \\
				0
			\end{pmatrix}
			,
			\begin{pmatrix}
				0 \\
				0 \\
				1
			\end{pmatrix}
		\right\}
	\]
	spans $\RR^3$.
\end{ex}

\begin{ex}
	Let
	\[
		W = \left\{
			\begin{pmatrix}
				a + b + 2c \\
				b + c \\
				a + c
			\end{pmatrix}
			\mid a,b,c \in \RR
		\right\}
		\subset \RR^3
	\]
	then
	\[
		w = a
		\begin{pmatrix}
			1 \\
			0 \\
			1
		\end{pmatrix}
		+ b
		\begin{pmatrix}
			1 \\
			1 \\
			0
		\end{pmatrix}
		+ c
		\begin{pmatrix}
			2 \\
			1 \\
			1
		\end{pmatrix}
		\forall w \in W
	\]
	\[
		\Rightarrow W = sp\left\{
			\begin{pmatrix}
				1 \\
				0 \\
				1
			\end{pmatrix}
			,
			\begin{pmatrix}
				1 \\
				1 \\
				0
			\end{pmatrix}
			,
			\begin{pmatrix}
				2 \\
				1 \\
				1
			\end{pmatrix}
		\right\}
	\]

	Also, since
	\[
		\begin{pmatrix}
			2 \\
			1 \\
			1
		\end{pmatrix}
		=
		\begin{pmatrix}
			1 \\
			0 \\
			1
		\end{pmatrix}
		+
		\begin{pmatrix}
			1 \\
			1 \\
			0
		\end{pmatrix},
	\]
	\[
		W = sp\left\{
			\begin{pmatrix}
				1 \\
				0 \\
				1
			\end{pmatrix}
			,
			\begin{pmatrix}
				1 \\
				1 \\
				0
			\end{pmatrix}
		\right\}
	\]
\end{ex}

\begin{defn}[Linear Independence]
	Let $V$ be a vector space. A set of vectors
	$\{v_1, \ldots, v_r \mid v_i \in V \}$
	is linearly independent if
	\[
		\alpha_1 v_1 + \cdots + \alpha_r v_r = 0
		\implies \alpha_1 = \cdots = \alpha_r = 0.
	\]
\end{defn}

\begin{thm}
	Let $v_1, \ldots, v_r \in \RR^n$ and $A$ be the matrix whose
	columns correspond to the vectors $v_i$, then the set
	$\{v_1, \ldots, v_r\}$ is linearly independent iff
	$A\bm{x} = 0$ has the unique solution $x = 0$ iff $\rho(A) = r$.
\end{thm}

\begin{proof}
	By definition of linear independence,
	\[
		x_1 v_1 + \cdots + x_r v_r = 0
		\iff
		x_i = 0 \, \forall i
	\]
	This is clearly equivalent to saying that $\bm x$ is the
	only solution to $A\bm x = 0$.

	A matrix equation like this can have a unique solution iff
	$\rho(A) = r$, otherwise there would be free variables resulting
	in a range of solutions.
\end{proof}

We now know that any matrix whose rank is equal to the number of its
rows forms a linearly independent spanning set for a subspace of
$\RR^n$.

\begin{cor}
	Any set of $m > n$ vectors in $\RR^n$ is linearly dependent.
\end{cor}

\begin{cor}
	The columns of the indentity matrix $I_n$ form a linearly
	independent spanning set of $\RR^n$.
\end{cor}

\begin{defn}[Basis]
	A subset $B$ of a vector space $V$ is called a basis of $V$ if $B$ is linearly independent and $B$ spans $V$.
\end{defn}

\begin{rem}
	When considering $\RR^n$ or its subsets, a basis will always be
	finite; however a basis can be infinite, for example
	$\{1, x, x^2, \ldots\}$ is a basis of $\RR[x]$.
\end{rem}

\begin{ex}
	\[
		\left\{
			\begin{pmatrix}
				1 \\
				2 \\
				3
			\end{pmatrix}
			,
			\begin{pmatrix}
				1 \\
				-1 \\
				0
			\end{pmatrix}
			,
			\begin{pmatrix}
				5 \\
				4 \\
				4
			\end{pmatrix}
		\right\}
	\]
	is a basis of $\RR^3$.

	\[
		\begin{pmatrix}
			1 & 1  & 5 \\
			2 & -1 & 4 \\
			3 & 0  & 4
		\end{pmatrix}
		=
		\begin{pmatrix}
			1 & 1  & 5  \\
			0 & -3 & -6 \\
			0 & -3 & -11
		\end{pmatrix}
		=
		\begin{pmatrix}
			1 & 1  & 5  \\
			0 & -3 & -6 \\
			0 & 0  & -5
		\end{pmatrix}
	\]

	We can see that the rank is $3$ hence this set is linearly independent.
	\[
		\begin{pmatrix}
			1 & 1  & 5  \\
			0 & -3 & -6 \\
			0 & 0  & -5
		\end{pmatrix}
		=
		\begin{pmatrix}
			1 & 1 & 5  \\
			0 & 1 & -2 \\
			0 & 0 & 1
		\end{pmatrix}
		=
		\begin{pmatrix}
			1 & 0 & 0 \\
			0 & 0 & 0 \\
			0 & 0 & 1
		\end{pmatrix}
	\]

	This shows that this set spans $\RR^3$ and is therefore a basis.
\end{ex}

\begin{thm}
	Let $V$ be a vector space over $\RR$. If $U = \{v_1, \ldots,
	v_r\}$ spans $V$ and no proper subset of $U$ spans $V$, then $U$
	is a basis for $V$.
\end{thm}

\begin{proof}
	We know that $U$ spans $V$ so we need only prove that $U$ is linearly independent. We will do so using the contrapositive method.

	If $U$ is not linearly independent then $\exists v_i \in U$ such that
	\[
		v_i = \alpha_1 v_1 + \cdots + \alpha_{i-1} v_{i-1} +
		\alpha_{i+1} v_{i+1} + \cdots + \alpha_r v_r
	\]
	\[
		\implies
		W = {v_1, \ldots, v_{i-1}, v_{i+1}, \ldots, v_r}
		\subset U
	\]
	spans $V$.

	Therefore, if there is no such $W$, $U$ must be linearly
	independent and hence a basis.
\end{proof}

\begin{thm}
	Let $V$ be a vector space over $\RR$. If $U = \{v_1, \ldots, v_r\}$
	is a linearly independent subset of $V$ and no proper, linearly
	independent  subset $W$ of $V$, where $W$ contains $U$, then $U$
	is a basis of $V$.
\end{thm}

\begin{proof}
	We know $U$ is linearly independent so we need only show that it
	spans $V$.

	Let $v \in V \setminus U$. According to our assumption that no
	linearly independent subset exists between $U$ and $V$, this
	means that $\{v_1, \ldots, v_r, v\}$ is  not linearly
	independent; hence any element in this set can be expressed as a
	linear combination of the rest.

	\[
		\implies
		v = \sum_{i=1}^r \alpha_i v_i \quad \forall v \in V
	\]

	Therefore, $U$ spans $V$ and $U$ is a basis for $V$.
\end{proof}

%TODO: look into Mac Lane generalisation of this lemma.
\begin{lemma}[Exchange (Steinitz)]
	Let $V$ be a vector space. Let $S = \{a_1, \ldots, \_s\}$ be a
	spanning set for a subspace $U \subset V$ and let $B = \{b_1,
	\ldots, b_r\}$ be a linearly independent subset of $U$. The $r
	\leq s$ and there exists a set $S_r$ containing $B$ and some
	elements from $S$ such that $U$ is spanned by $S_r$.
\end{lemma}

%TODO: be consistent with sp and span, declare them as operators:
%\DeclareMathOperator\span{span}
\begin{proof}
	\[
		b_1 \in U = sp(S)
		\implies b_1 = \sum_{i=1}^s \alpha_i a_i,
	\]
	where not all $a_i = 0$ (else $b_1 = 0$).
	\[
		\implies a_1 = \alpha_1^{-1}
		( b_1 - \alpha_2 a_2 - \cdots - \alpha_s a_s )
	\]

	Assume we have replaced $(a_1, \ldots, a_k)$ with
	$(b_1, \ldots, b_k)$ in this manner, so that
	$U$ is spanned by $\{ b_1, \ldots, b_k, a_{k+1}, \ldots, a_s \}$.

	Exchange $b_{k+1}$:
	\[ b_{k+1} \in U \]
	\[
		\implies b_{k+1} =
		\beta_1 b_1 + \cdots + \beta_k b_k
		+ \alpha_{k+1} a_{k+1} + \cdots + \alpha_s a_s
	\]
	again, not all coefficients are zero.

	At least one $\alpha_i \neq 0$ else $b_{k+1}$ would be a linear
	combination of $\{ b_1, \ldots, b_k \}$.

	\[
		\implies \alpha_{k+1} = \alpha_{k+1}^{-1}
		(
			b_{k+1} - \beta_1 b_1 - \cdots - \beta_k b_k
			- \alpha_{k+2} a_{k+2} - \cdots - \alpha_s a_s
		)
	\]

	So for all $1 \leq k \leq r$ we can replace $a_k$ with $b_k$, giving:
	\[
		U = span \{ b_1, \ldots, b_{k+1}, a_{k+2}, \ldots, a_s \}
	\]
\end{proof}

This result is significant because when we have a basis, $B$, for a
vector space, $V$, it can be used as the linearly independent set or the
spanning set. This tells us that any spanning set for $V$ has a greater
or equal cardinality compared with $B$ and that any linearly independent
subset of $V$ has a cardinality smaller than or equal to that of $B$.
The next result follows from this.

\begin{cor}
	Any two bases for a vector space have the same number elements.
\end{cor}

\begin{defn}[Dimension]
	The number of elements in a basis for a vector space is the dimension of the vector space.
\end{defn}

\begin{rem}
	Let $U$ be a vector space with dimension $n$ and $S \subset U$
	such that $|S| = n$. If $S$ spans $U$ or if $S$ is linearly
	independent, then $S$ must be a basis of $U$.
\end{rem}

Rank and Nullity

\begin{defn}[Nullity]
	The nullity of a matrix $A$ is the dimension of its null space.
\end{defn}

\begin{defn}[Rowspace]
	Let $A \in M_{m \times n}$ an $a_{ij}$ be the $i^{th}$ row of $A$.
	The rowspace of $A$ is:
	\[
		sp \{ a_{1j}, \ldots, a{nj} \}
	\]
\end{defn}

Note that the dimension of the rowspace is the number of non-zero rows
in echelon form, i.e.\ the (row) rank. We can refer to the column rank as
the dimension of the column/image space.

\begin{thm}[Row rank equals column rank]
	Let $A \in M_{m \times n}$, then $\rho(A) = \rho(A^T)$. In other
	words, the row rank and column rank are equal.
\end{thm}

\begin{proof}
	Let the column rank of $A$ be $r = \rho(A^T)$. Let $C$ be the
	matrix whose columns form a basis for the column space of $A$.

	Every column of $A$ can be expressed as a linear combination of
	the columns in $C$ hence there is a matrix $R$ such that
	$A = CR$.

	This means that every column of $A$ can be given as a linear
	combination of the rows of $R$, which implies that it is a
	spanning set for the row space of $A$.

	There are $r$ columns in $C$, hence $r$ rows in $R$ so, by the
	Exchange Lemma, we know that $\rho(A) \leq r = \rho(A^T)$.

	We can apply this same process to the matrix $A^T$, where the
	rows and columns (and hence their ranks) are swapped. From this
	we see that $\rho(A^T) \leq \rho({(A^T)}^T) = \rho(A)$.

	\[
		\implies \rho(A) = \rho(A^T)
	\]
\end{proof}

\begin{thm}
	Let $A \in M_n(\RR)$ with $Ax = b$, then the following statement
	are equivalent:
	\begin{enumerate}
		\item
			$Ax = b$ has a unique solution
		\item
			$\rho(A) = n$
		\item
			$A$ is equivalent to $I_n$
		\item
			$A$ is invertible
	\end{enumerate}
\end{thm}

\begin{proof}
	We have already seen $1 \iff 2$ and $2 \iff 3$ is obvious
	considering the RRE form of $A$.

	$3 \implies 4$ because by reducing $A$ to $I_n$, we can use the
	product of the row operations to construct the inverse matrix.

	$4 \implies 3$:
	\[ \exists A^{-1} \implies A^{-1}A = I_n \]
	$A^{-1}$ can be split into row operations that, when applied to
	$A$, yields $I_n$.
\end{proof}

\begin{defn}
	The $(i,j)$-minor of $A \in M_n$ is the $(n-1)\times(n-1)$
	matrix obtained by removing the $i^{th}$ row and $j^{th}$
	column. It is often denoted $M_{i,j}$.
\end{defn}

\begin{defn}[Determinant]
	The determinant is defined recursively:
	\[ n=1 \implies \det [a] = a \]
	\[ n=k \implies \det A = \sum_{j=1}^k a_{1,j} C_{1,j} \]
	where
	\[ C_{i,j} = {(-1)}^{i+j} \det(M_{i,j}) \]
	is the $(i,j)$-cofactor.
\end{defn}

Properties of the Determinant

\begin{enumerate}
	\item
		If any row is all zeros, $\det A = 0$
	\item
		If $B$ is obtained by adding one row of $A$ to a
		different one, $\det A = \det B$
	\item
		If $B$ is obtained by swapping one row of $A$ with
		another, $\det A = -\det B$
	\item
		If $B$ is obtained by multiplying one row of $A$ by
		$\alpha$, $\alpha \det A = \det B$
	\item
		If $A$ is upper triangular, then $\det A$ is the product
		of the diagonal elements
	\item
		$\det(A^T) = \det(A)$
	\item
		$\det(AB) = \det(A) \det(B)$
\end{enumerate}

We will not prove most of these here, but property 5 can be deduced
simply enough using the others and induction.

\begin{proof}
The $n=1$ case is obvious, so we may assume the property holds for
$n=k$.

\[
	A =
	\begin{bmatrix}
		a_{1,1}   & \cdots & a_{1,k}   & a_{1,k+1} \\
		\vdots    & \ddots & \vdots    & \vdots    \\
		a_{k,1}   & \cdots & a{k,k}    & a_{k,k+1} \\
		a_{k+1,1} & \cdots & a_{k+1,k} & a_{k+1,K+1}
	\end{bmatrix}
\]

The other row properties allow us to traverse any row in order to
calculate the determinant, so we can choose the bottom row. This is all
zeros except for the element $a_{k+1,k+1}$. We can see that
\[
	\det A = a_{k+1,K+1} \det A_k
\]
which is the product of the diagonals.
\end{proof}

Some futher properties can be deduced from those we have already described:
\begin{itemize}
	\item
		If any two rows are identical then $\det A = 0$ (using 1,2,4)
	\item
		All row properties also apply to columns (due to 6)
	\item
		The determinant is commutative (due to 7)
	\item
		$\det(\alpha A) = \alpha^n \det A$ (due to 4)
	\item
		$\det I_n = 1$ (using 5)
\end{itemize}

\begin{defn}[Eigenvalues and Eigenvectors]
	Let $A \in M_n(\RR)$ and $\lambda \in \RR$. If $\exists x \neq
	0$ such that $Ax = \lambda x$, then $\lambda$ is called an
	Eigenvalue of $A$ and $x$ is called an Eigenvector of $A$.
\end{defn}

\begin{lemma}
	$\exists A^{-1} \iff |A| \neq 0$
\end{lemma}

%TODO: deduce the method for calculating the inverse (springer)
\begin{proof}
	Assume $\exists A^{-1}$
	\[ \implies |A^{-1}A| = |I| = 1 \]
	\[ \implies |A^{-1}||A| = 1 \]
	\[ \implies |A| \neq 0 \]

	Assume $|A| \neq 0$. This means we can use the standard method
	to calculate the inverse.
\end{proof}

\begin{lemma}
	$\lambda$ is an Eigenvalue $\iff |A - \lambda I_n| = 0$.
\end{lemma}

\begin{proof}
	\[ Ax = \lambda x \iff Ax = \lambda I_n x \iff (A - \lambda I_n)x = 0 \]

	By definition, $x \neq 0$ hence $(A - \lambda I_n)$ cannot have an inverse.

	\[ \iff |A - \lambda I_n| = 0 \]
\end{proof}

\begin{defn}[Characteristic Equation]
	$|A - \lambda I_n| = 0$ is called the characteristic equation of
	$A$. Solving it for $\lambda$ yields eigenvalues for $A$, which
	can be used to find eigenvectors from $(A - \lambda I_n)x = 0$.
\end{defn}

Linear Transformations

\begin{defn}[Linear Transformation]
	Let $V$ and $W$ be vector spaces over $\RR$. A function $T:V \to
	W$ is a linear transformation if $\forall v_1, v_2 \in V, \alpha
	\in \RR$ we have
	\begin{enumerate}
		\item
			$T(v_1 + v_2) = T(v_1) + T(v_2)$
		\item
			$T(\alpha v_1) = \alpha T(v_1)$
	\end{enumerate}
\end{defn}

\begin{defn}[Kernel]
	\[ \ker T = \{ v \in V \mid T(v) = 0 \} \]
\end{defn}

\begin{defn}
	\[ \im T = \{ w \in W \mid w = T(v), v \in V \} \]
\end{defn}

\begin{prop}
	$\ker T$ is a subspace of $V$ and $\im T$ is a subspace of $W$.
\end{prop}

\begin{proof}
	\[ T(0) = T(v-v) = T(v) - T(v) = 0 \implies 0 \in \ker T \]

\end{proof}


\end{document}
